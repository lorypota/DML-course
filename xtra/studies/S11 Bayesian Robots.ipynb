{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T08:06:54.936445Z",
     "start_time": "2019-11-06T08:06:54.934166Z"
    }
   },
   "source": [
    "# Bayesian Robots!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Bayesian optimization to efficiently tune machine learning algorithms for robot movement.\n",
    "\n",
    "### Robot Navigation\n",
    "The [Wall robot navigation](https://www.openml.org/d/1497) contains training data for a robot equiped with ultrasound sensors. Based on 24 sensor readings, the robot has to figure out how to move though an office space without hitting walls or other obstacles. The possible actions (classes) are 'Move-Forward', 'Slight-Right-Turn', 'Sharp-Right-Turn', and 'Slight-Left-Turn'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "%matplotlib inline\n",
    "import openml as oml\n",
    "\n",
    "# Download Wall Robot Navigation data from OpenML.\n",
    "robotnav = oml.datasets.get_dataset(1497)\n",
    "X, y, cats, attrs = robotnav.get_data(dataset_format='array',\n",
    "    target=robotnav.default_target_attribute)\n",
    "labels = ['Move-Forward','Slight-Right-Turn','Sharp-Right-Turn','Slight-Left-Turn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data\n",
    "Let's try to plot the position of the robot and where the walls are according to the sensors.\n",
    "We can compute the coordinates of the detected wall using the angle of each sensor and basic\n",
    "geometry:  \n",
    "\n",
    "$ x_{wall} = x_{robot} + dist * cos(angle_{robot} + angle_{sensor}) $  \n",
    "$ y_{wall} = y_{robot} + dist * sin(angle_{robot} + angle_{sensor}) $\n",
    "\n",
    "Where $x_{robot}$ is the x-coordinate of the robot, $dist$ is the distance measured by the sensor,\n",
    "$angle_{robot}$ is the current direction the robot is facing and $angle_{sensor}$ is the relative\n",
    "angle of the specific sensor.\n",
    "\n",
    "The dataset and the paper do not give any information on how fast the robot moves and how fast it\n",
    "turns, so we have to guess these. It does say that it measures 9 samples per second. After some\n",
    "trial and error we get plausible results if we set the speed to 0.1 meter per second and the turning\n",
    "rate to about 2 degrees per second. Below is the resulting animation in which the robot is presented\n",
    "as a triangle (green when moving and red when turning). The dots are the assumed locations of walls.\n",
    "Although the visualization is probably not very precise, we can see that the robot follows the nearest wall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import colors\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "matplotlib.rcParams['animation.embed_limit'] = 100\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_tight_layout(True)\n",
    "cx, cy = 0, 0 # robot position\n",
    "angle = 180   # robot direction\n",
    "ax.clear()\n",
    "robot = ax.scatter(cx,cy, color='r', s=100, marker=(3, 0, angle+30))\n",
    "\n",
    "def update(n): \n",
    "    global angle, cx, cy, robot\n",
    "    curr_x, cl = X[n], y[n]\n",
    "    if cl==0:\n",
    "        cx+=math.cos(math.radians(angle))*0.012\n",
    "        cy+=math.sin(math.radians(angle))*0.012\n",
    "    elif cl==1:\n",
    "        angle -= 0.02\n",
    "    elif cl==2:\n",
    "        angle -= 0.9\n",
    "    elif cl==3:\n",
    "        angle += 0.02\n",
    "    if n%30==0: #speed things up by only plotting every n'th step\n",
    "        wall_points =np.array([[cx+dist*math.cos(math.radians(angle-180+i*15)),cy+dist*math.sin(math.radians(angle-180+i*15))] \n",
    "                               for i, dist in enumerate(curr_x) if dist<2 ])\n",
    "        ax.clear()\n",
    "        ax.set_xlim(-10,5)\n",
    "        ax.set_ylim(-2,10)\n",
    "        #robot.remove();\n",
    "        robot = ax.scatter(cx,cy, color=('g' if cl==0 else 'r'), s=100, marker=(3, 0, angle+30))\n",
    "        ax.scatter(wall_points[:,0],wall_points[:,1], color='k', s=1)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = FuncAnimation(fig, update, frames=np.arange(0, 5000), interval=1)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification models\n",
    "We try the following models (for simplicity, we will only tune the 2 most important hyperparameters per model):  \n",
    "* Support vector machine, with hyperparameters $C \\in [10^{-12},10^{12}]$, $\\gamma \\in [10^{-12},10^{12}]$ (both log scale), and an RBF kernel.\n",
    "* Gradient Boosting, with hyperparameters `learning_rate` $\\in [10^{-4},10^{-1}]$ (log scale), `max_depth` $\\in [1,5]$, and `n_estimators` fixed at least 1000 (you can increase this if your computing resources allow).\n",
    "\n",
    "We want a fast way to optimize and re-optimize the model so that it will keep working well. We will use Bayesian optimization for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T10:53:38.524802Z",
     "start_time": "2019-11-06T10:53:36.394068Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "import imageio as io\n",
    "import itertools\n",
    "import os\n",
    "import time\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "from random import randint\n",
    "from scipy.stats import norm\n",
    "from matplotlib import colors\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from xgboost.sklearn import XGBModel\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T10:53:38.579462Z",
     "start_time": "2019-11-06T10:53:38.527210Z"
    },
    "code_folding": [
     1,
     15
    ]
   },
   "outputs": [],
   "source": [
    "# Random Forest that also returns the standard deviation of predictions\n",
    "class ProbRandomForestRegressor(RandomForestRegressor):\n",
    "    \"\"\"\n",
    "    A Random Forest regressor that can also returns the standard deviations for all predictions\n",
    "    \"\"\"\n",
    "    def predict(self, X, return_std=True):       \n",
    "        preds = []\n",
    "        for pred in self.estimators_:\n",
    "            preds.append(pred.predict(X))\n",
    "        if return_std:\n",
    "            return np.mean(preds, axis=0), np.std(preds, axis=0)\n",
    "        else:\n",
    "            return np.mean(preds, axis=0)\n",
    "\n",
    "# Helper function to compute expected improvement \n",
    "def EI(surrogate, X: np.ndarray, curr_best=0.0, balance=0.0, **kwargs):\n",
    "    \"\"\"Computes the Expected Improvement\n",
    "    surrogate, The surrogate model\n",
    "    X: np.ndarray(N, D), The input points where the acquisition function\n",
    "        should be evaluated. N configurations with D hyperparameters\n",
    "    curr_best, The current best performance\n",
    "    balance, Decrease to focus more on exploration, increase to focus on exploitation (optional)\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray(N,1), Expected Improvement of X\n",
    "    \"\"\"\n",
    "    if len(X.shape) == 1:\n",
    "        X = X[:, np.newaxis]\n",
    "\n",
    "    m, s = surrogate.predict(X, return_std=True) # mean, stdev\n",
    "\n",
    "    z = (curr_best - m - balance) / s\n",
    "    f = (curr_best - m - balance) * norm.cdf(z) + s * norm.pdf(z)\n",
    "\n",
    "    if np.any(s == 0.0): # uncertainty should never be exactly 0.0\n",
    "        f[s == 0.0] = 0.0\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T10:53:38.611665Z",
     "start_time": "2019-11-06T10:53:38.582199Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fetch classification data\n",
    "robotnav = oml.datasets.get_dataset(1497)\n",
    "X, y, _, _ = robotnav.get_data(dataset_format='array',\n",
    "    target=robotnav.default_target_attribute)\n",
    "\n",
    "# Fetch regression data\n",
    "robotarm = oml.datasets.get_dataset(189)\n",
    "Xr, yr, _, _ = robotarm.get_data(dataset_format='dataframe',\n",
    "    target=robotarm.default_target_attribute)\n",
    "\n",
    "X.shape, y.shape, Xr.shape, yr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementing Bayesian Optimization (60 points) {-}\n",
    "* Implement Bayesian optimization using the code above and use it to optimize the hyperparameters stated below for each of the two datasets.\n",
    "    - Use the hyperparameters and ranges are defined above. Make sure to sample from a log scale (`numpy.logspace`) whenever the hyperparameters should be varied on a log scale. \n",
    "    - The evaluation measure for classification should be misclassification error (1 - Accuracy), evaluated using 3-fold cross-validation\n",
    "    - The evaluation measure for regression should be mean squared error, also evaluated using 3-fold cross-validation\n",
    "* Initialize the surrogate model with 10 randomly sampled configurations and visualize the surrogate model.\n",
    "    - Hint: Use a 2D slice of each hyperparameter (e.g. $C$=4 and $\\gamma$=0.1) to show both the predicted values and the uncertainty.\n",
    "    - For simplicity, you can build a separate surrogate model for each algorithm and each dataset (4 models in total) \n",
    "* Visualize the resulting acquisition function, either as 2D slices (or, more difficult, as a 3D surface)\n",
    "* Visualize 3 more iterations, each time visualizing the surrogate model and acquisition function\n",
    "* Run the Bayesian optimization for at least 30 iterations, report the optimal configuration and show the final surrogate model (2D slices or 3D surface).\n",
    "* Interpret and explain the results. Does Bayesian optimization efficiently find good configurations? Do you notice any\n",
    "differences between the different models and the different datasets. Explain the results as well as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class to perform the Bayesian Optimization\n",
    "You can supply your own surrogate and objective function to the model. The hyperparameter combinations we would like to run on the objective model are stored in `self.hyperparam_obj`. After the objective model calculated the losses we create a grid of 30 by 30 of all hyperparameter combinations. This is stored in `self.hyperparams_sur`. Afterwards we predict the loss for all these values and create a 3D plot to show the surrogate model and acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:34:30.355241Z",
     "start_time": "2019-11-06T12:34:30.309530Z"
    },
    "code_folding": [
     1,
     32,
     38,
     63,
     88,
     109,
     131,
     153,
     192,
     209,
     219,
     313,
     329
    ]
   },
   "outputs": [],
   "source": [
    "class BayesianOptimization:\n",
    "    def __init__(self, \n",
    "               surrogate_model, \n",
    "               objective_model, \n",
    "               acquisition_function,\n",
    "               hyperparam_space,\n",
    "               X_obj,\n",
    "               y_obj):\n",
    "        \n",
    "        self.surrogate_model = surrogate_model\n",
    "        self.objective_model = objective_model\n",
    "        self.hyperparam_names = [*hyperparam_space.keys()]\n",
    "        self.hyperparam_space = hyperparam_space\n",
    "        self.acquisition_function = acquisition_function\n",
    "        self.X_obj=X_obj\n",
    "        self.y_obj=y_obj\n",
    "        self.highest_EI=0\n",
    "        self.model_name = type(objective_model()).__name__\n",
    "        self.surrogate_model_name = type(surrogate_model).__name__\n",
    "        self.time_surrogate=[]\n",
    "\n",
    "        # Dataframe to store all hyperparameters and their predictions\n",
    "        # of the objective function\n",
    "        self.hyperparams_obj = pd.DataFrame(\n",
    "          columns=[*self.hyperparam_names, 'loss'])\n",
    "\n",
    "        # Dataframe to store all hyperparameters and their predictions\n",
    "        # of the surrogate model\n",
    "        self.hyperparams_sur = pd.DataFrame(\n",
    "          columns=[*self.hyperparam_names, 'pred', 'sigma', 'improvement'])\n",
    "        \n",
    "    def optimal_hyperparams(self, rows=1):\n",
    "        \"\"\"\n",
    "        Returns dataframe with the optimal hyperparameters and the corresponding loss score. \n",
    "        \"\"\"\n",
    "        return self.hyperparams_obj.sort_values('loss')[:rows]\n",
    "    \n",
    "    def hyperparam_points(self, hyperparam_name, size, unique=False, discrete=True):\n",
    "        \"\"\"\n",
    "        Create a population for a certain hyperparameter\n",
    "        (E.g. 1000 log spaced points to sample from)\n",
    "        \"\"\"\n",
    "        # Get the required settings\n",
    "        settings = self.hyperparam_space[hyperparam_name]\n",
    "        \n",
    "        # Log spaced\n",
    "        if settings['spacing'] == 'log':\n",
    "            return np.logspace(settings['min'], settings['max'], size)\n",
    "        \n",
    "        # Linear spaced\n",
    "        if settings['spacing'] == 'lin':\n",
    "            # Only unique values (E.g. 1,2,3,4,5)\n",
    "            if unique and discrete:\n",
    "                return np.arange(settings['min'], settings['max']+1, 1).astype(int)\n",
    "            \n",
    "            # Only discrete values (E.g. 1,1,1,1,1,1,2,2,2,2,2)\n",
    "            if discrete:\n",
    "                return np.round(np.linspace(settings['min'], settings['max'], size)).astype(int)\n",
    "            \n",
    "            # Continues values (E.g. 1.0, 1.1, 1.2, 1.3, 1.4)\n",
    "            return np.linspace(settings['min'], settings['max'], size)\n",
    "    \n",
    "    def hyperparam_cartesian(self, size):\n",
    "        \"\"\"\n",
    "        Create a grid of the two hyperparameters.\n",
    "        \"\"\"\n",
    "        points = []\n",
    "\n",
    "        for hyperparam_name in self.hyperparam_names:\n",
    "            settings = self.hyperparam_space[hyperparam_name]\n",
    "            \n",
    "            # Check if the sampling needs to be discrete\n",
    "            discrete = ((settings['spacing'] == 'lin') & settings['discrete'])\n",
    "            \n",
    "            # Get the population of points\n",
    "            points.append(self.hyperparam_points(hyperparam_name, size, unique=True, discrete=discrete))\n",
    "\n",
    "        # Create the cartesian product of the two hyperparams\n",
    "        cartesian = np.array(np.meshgrid(*points)).T.reshape(-1,2)\n",
    "\n",
    "        # Store the values\n",
    "        for idx, hyperparam_name in enumerate(self.hyperparam_names):\n",
    "                \n",
    "            self.hyperparams_sur[hyperparam_name] = cartesian[:,idx]\n",
    "\n",
    "        return self.hyperparams_sur\n",
    "      \n",
    "    def hyperparam_sampling(self, n_samples=10):\n",
    "        \"\"\"\n",
    "        Create the initial hyperparameter samples to start the optimization.\n",
    "        \"\"\"\n",
    "        for hyperparam_name in self.hyperparam_names:\n",
    "            settings = self.hyperparam_space[hyperparam_name]\n",
    "            \n",
    "            # Check if the sampling needs to be discrete\n",
    "            discrete = ((settings['spacing'] == 'lin') & settings['discrete'])\n",
    "            \n",
    "            # Get the population of points\n",
    "            points = self.hyperparam_points(hyperparam_name, 1000, discrete=discrete)\n",
    "\n",
    "            # Take n samples from the population\n",
    "            sample = points[np.random.choice(1000, n_samples)]\n",
    "\n",
    "            # Store the results\n",
    "            self.hyperparams_obj[hyperparam_name] = sample\n",
    "\n",
    "        return self.hyperparams_obj\n",
    "    \n",
    "    def \n",
    "    \n",
    "    (self, **static_hyperparams):\n",
    "        \"\"\"\n",
    "        Predict all the hyperparameter combinations that do not have a prediction yet\n",
    "        \"\"\"\n",
    "        for idx, hyperparams in (self.hyperparams_obj[self.hyperparams_obj['loss'].isnull()]\n",
    "                                 .drop('loss', axis=1)\n",
    "                                 .to_dict('index')\n",
    "                                 .items()):\n",
    "            \n",
    "            # Setup the classifier\n",
    "            clf = self.objective_model(**hyperparams, **static_hyperparams)\n",
    "\n",
    "            # Get the mean accuracy over all three folds\n",
    "            scores = cross_val_score(clf, self.X_obj, self.y_obj, cv=3, n_jobs=-1, \n",
    "                                       scoring=\"accuracy\")\n",
    "\n",
    "            # Calculate the mean 1 - acc loss\n",
    "            loss = np.mean(1 - np.array(scores))\n",
    "\n",
    "            # Store the loss\n",
    "            self.hyperparams_obj.loc[idx, 'loss'] = loss\n",
    "      \n",
    "    def regressor_predict(self, **static_hyperparams):\n",
    "        \"\"\"\n",
    "        Predict all the hyperparameter combinations that do not have a prediction yet\n",
    "        \"\"\"\n",
    "        for idx, hyperparams in (self.hyperparams_obj[self.hyperparams_obj['loss'].isnull()]\n",
    "                                 .drop('loss', axis=1)\n",
    "                                 .to_dict('index')\n",
    "                                 .items()):\n",
    "            \n",
    "            # Setup the classifier\n",
    "            clf = self.objective_model(**hyperparams, **static_hyperparams)\n",
    "\n",
    "            # Get the mean accuracy over all three folds\n",
    "            scores = cross_val_score(clf, self.X_obj, self.y_obj, cv=3, n_jobs=-1, \n",
    "                                       scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "            # Calculate \n",
    "            loss = np.mean(np.array(scores) * -1)\n",
    "\n",
    "            # Store the loss\n",
    "            self.hyperparams_obj.loc[idx, 'loss'] = loss\n",
    "      \n",
    "    def surrogate_predict(self, **static_hyperparams):\n",
    "        \"\"\"\n",
    "        Predict all the hyperparameter combinations using the surrogate model\n",
    "        \"\"\"\n",
    "        # The train set will be the hyperparameters that have been\n",
    "        # predicted by the objective model\n",
    "        X_sur = self.hyperparams_sur[list(self.hyperparam_names)]\n",
    "        X_train = self.hyperparams_obj[list(self.hyperparam_names)]\n",
    "        y_train = self.hyperparams_obj['loss']\n",
    "        \n",
    "        # Record the start time\n",
    "        start = time.time()\n",
    "        \n",
    "        # Fit the model on the objective model losses\n",
    "        self.surrogate_model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict all points using the surrogate model\n",
    "        y_pred, sigma = self.surrogate_model.predict(\n",
    "          X_sur, \n",
    "          return_std=True)\n",
    "        \n",
    "        # Record the end time\n",
    "        end = time.time()\n",
    "        self.time_surrogate.append(end-start)\n",
    "        \n",
    "        self.hyperparams_sur.loc[:, 'pred'] = y_pred\n",
    "        self.hyperparams_sur.loc[:, 'sigma'] = sigma\n",
    "\n",
    "        # Calculate the expected improvement\n",
    "        expected_improvement = self.acquisition_function(\n",
    "          self.surrogate_model, \n",
    "          X_sur)\n",
    "        \n",
    "        # Store the max expected improvement for creating the graphs\n",
    "        if np.max(expected_improvement) > self.highest_EI:\n",
    "            self.highest_EI = np.max(expected_improvement)\n",
    "        \n",
    "        self.hyperparams_sur.loc[:, 'improvement'] = expected_improvement\n",
    "  \n",
    "    def next_sample(self):\n",
    "        \"\"\"\n",
    "        Function that picks the next best hyperparameters to look at\n",
    "        \"\"\"\n",
    "        # Get the index of the hyperparameters that show the highest expected improvement\n",
    "        # We shuffle the dataframe to prevent it from picking the same values over and over again\n",
    "        parameters_idx = self.hyperparams_sur['improvement'].sample(frac=1).idxmax()\n",
    "\n",
    "        # Get the values of hyperparameters associated with highest expected improvement\n",
    "        opt_params = self.hyperparams_sur.loc[parameters_idx, list(self.hyperparam_names)]\n",
    "\n",
    "        # Add it to the objective model hyperparameters to be predicted in the next run\n",
    "        self.hyperparams_obj = self.hyperparams_obj.append(opt_params, ignore_index=True)\n",
    "        \n",
    "        # Change the data types to fix a bug with XGBoost\n",
    "        self.force_dtypes()\n",
    "  \n",
    "    def force_dtypes(self):\n",
    "        \"\"\"\n",
    "        Helper fucntion to cast the column of the hyperparams objective function to the right types\n",
    "        \"\"\"\n",
    "        for hyperparam_name in self.hyperparam_names:\n",
    "            setting = self.hyperparam_space[hyperparam_name]\n",
    "            \n",
    "            if (setting['spacing'] == 'lin') and setting['discrete']:\n",
    "                self.hyperparams_obj[hyperparam_name] = self.hyperparams_obj[hyperparam_name].astype(int)\n",
    "        \n",
    "    def plot_surrogate(self, label='', show_confidence=False, show_plot=True, store_plot=False):\n",
    "        \"\"\"\n",
    "        Function to create a 3d plot of the surrogate model and the acquisition \n",
    "        \"\"\"\n",
    "\n",
    "        # Create the two figures\n",
    "        fig = plt.figure(figsize=(16, 6))\n",
    "        ax_sur = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "        ax_acq = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "\n",
    "        # Get the z values\n",
    "        z_loss = self.hyperparams_sur['pred']\n",
    "        z_loss_obj = self.hyperparams_obj['loss'].astype(float) + 0.03\n",
    "        z_acq = self.hyperparams_sur['improvement']\n",
    "\n",
    "        first_param = self.hyperparam_space[self.hyperparam_names[0]]\n",
    "        second_param = self.hyperparam_space[self.hyperparam_names[1]]\n",
    "\n",
    "        x_plot = self.hyperparams_sur[self.hyperparam_names[0]]\n",
    "        y_plot = self.hyperparams_sur[self.hyperparam_names[1]]\n",
    "\n",
    "        x_plot_obj = self.hyperparams_obj[self.hyperparam_names[0]]\n",
    "        y_plot_obj = self.hyperparams_obj[self.hyperparam_names[1]]\n",
    "\n",
    "        # Space the points logaritmic if necessary\n",
    "        if first_param['spacing'] == 'log':\n",
    "            x_plot = np.log10(x_plot)\n",
    "            x_plot_obj = np.log10(x_plot_obj)\n",
    "        if second_param['spacing'] == 'log':\n",
    "            y_plot = np.log10(y_plot)\n",
    "            y_plot_obj = np.log10(y_plot_obj)\n",
    "\n",
    "        # The maximum loss of the surrogate function\n",
    "        loss_max = self.hyperparams_sur['pred'].max()\n",
    "        loss_max += (loss_max * 0.1)\n",
    "\n",
    "        # Show the plots\n",
    "        ax_sur.plot_trisurf(x_plot, y_plot, z_loss, cmap='plasma', zorder=1, alpha=0.5)\n",
    "\n",
    "        for index in range(len(x_plot_obj)):\n",
    "            first_hyperparam = x_plot_obj[index]\n",
    "            second_hyperparam = y_plot_obj[index]\n",
    "            loss = z_loss_obj[index]\n",
    "\n",
    "            ax_sur.plot(\n",
    "                [first_hyperparam,first_hyperparam],\n",
    "                [second_hyperparam,second_hyperparam],\n",
    "                [0,loss_max*0.05],\n",
    "                color = 'black', linewidth=1.2, zorder=0)\n",
    "\n",
    "        ax_acq.plot_trisurf(x_plot, y_plot, z_acq, cmap='plasma')\n",
    "\n",
    "        if first_param['spacing'] == 'log':\n",
    "            min, max = first_param['min'], first_param['max']\n",
    "            plt.xticks(np.linspace(min,max,5), np.logspace(min,max,5))\n",
    "            ax_sur.xaxis.set_major_formatter(mtick.FormatStrFormatter('%.2e'))\n",
    "            ax_acq.xaxis.set_major_formatter(mtick.FormatStrFormatter('%.2e'))\n",
    "\n",
    "        ax_sur.set_zlim(0, loss_max)\n",
    "        ax_acq.set_zlim(0, self.highest_EI)\n",
    "\n",
    "        # Labels\n",
    "        ax_sur.set_title('Hyper-parameter loss function for {} {}'.format(\n",
    "            self.model_name, label), pad=30, fontsize=12)\n",
    "        ax_acq.set_title('Hyper-parameter expected improvement function for {} {}'.format(\n",
    "            self.model_name, label), pad=30, fontsize=12)\n",
    "\n",
    "        ax_sur.set_xlabel(self.hyperparam_names[0], fontsize=10)\n",
    "        ax_sur.set_ylabel(self.hyperparam_names[1], fontsize=10)\n",
    "        ax_sur.set_zlabel('loss', fontsize=10)\n",
    "        ax_acq.set_xlabel(self.hyperparam_names[0], fontsize=10)\n",
    "        ax_acq.set_ylabel(self.hyperparam_names[1], fontsize=10)\n",
    "        ax_acq.set_zlabel('Expected Improvement', fontsize=10)\n",
    "\n",
    "        # Give the axis labels a bit more space\n",
    "        ax_sur.xaxis.labelpad, ax_sur.yaxis.labelpad, ax_sur.zaxis.labelpad = 10, 10, 10\n",
    "        ax_acq.xaxis.labelpad, ax_acq.yaxis.labelpad, ax_acq.zaxis.labelpad = 10, 10, 10\n",
    "\n",
    "        # Alter font sizes\n",
    "        plt.rcParams.update({'font.size': 14})\n",
    "        ax_sur.tick_params(axis='both', which='major', labelsize=10)\n",
    "        ax_sur.tick_params(axis='both', which='minor', labelsize=8)\n",
    "        ax_acq.tick_params(axis='both', which='major', labelsize=10)\n",
    "        ax_acq.tick_params(axis='both', which='minor', labelsize=8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if store_plot:\n",
    "            plt.savefig('.gif_images/{}_it_{}.png'.format(self.model_name, len(self.hyperparams_obj)))\n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close(fig)\n",
    "            \n",
    "    def generate_gif(self):\n",
    "        \"\"\"\n",
    "        Function to generate a gif based on the loss plots of the model\n",
    "        \"\"\"\n",
    "        png_dir = '.gif_images/'\n",
    "        images = []\n",
    "        \n",
    "        for file_name in sorted(os.listdir(png_dir)):\n",
    "            if file_name.startswith(self.model_name):\n",
    "                file_path = os.path.join(png_dir, file_name)\n",
    "                images.append(io.imread(file_path))\n",
    "                gif_filename = '{}{}.gif'.format(png_dir, self.model_name)\n",
    "                io.mimsave(gif_filename, images, duration=.5)\n",
    "                \n",
    "        return Image(filename=gif_filename)\n",
    "    \n",
    "    def plot_loss(self):\n",
    "        \"\"\"\n",
    "        Plots the minimum loss over 30 iterations\n",
    "        \"\"\"\n",
    "        # List for the lowest loss score observed per iteration\n",
    "        min_points = []\n",
    "\n",
    "        # Appends the lowest loss from the initial 10 samples\n",
    "        min_points.append(np.min(self.hyperparams_obj[:10]['loss']))\n",
    "\n",
    "        # Iteratively appends the lowest loss\n",
    "        for iteration in range(len(self.hyperparams_obj[10:])):\n",
    "            min_points.append(np.min(self.hyperparams_obj[:iteration+11]['loss']))\n",
    "        \n",
    "        # Lowest loss score overall\n",
    "        min_loss = np.min(min_points)\n",
    "        \n",
    "        # Show plot of minimum loss over the iterations\n",
    "        fig, ax = plt.subplots(figsize=(20,6))\n",
    "\n",
    "        textstr = 'minimum loss = {:.5f}'.format(min_loss)\n",
    "\n",
    "        plt.plot(range(0, len(min_points)), min_points, '--', range(0, len(min_points)), min_points, 'bo')\n",
    "        plt.title('Minimum loss over 30 iterations for {}'.format(self.model_name))\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        textstr = 'minimum loss = {:.5f}'.format(min_loss)\n",
    "\n",
    "        # place a text box in upper right in axes coords\n",
    "        ax.text(0.80, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n",
    "                verticalalignment='top', bbox=props) \n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 random samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "\n",
    "np.logspace(settings['min'], settings['max'], size)\n",
    "\n",
    "def initialize:\n",
    "    print()\n",
    "    \n",
    "\n",
    "\n",
    "def update(n): \n",
    "    ax.scatter(wall_points[:,0],wall_points[:,1], color='k', s=1)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = FuncAnimation(fig, update, frames=np.arange(0, 30), interval=100)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T11:20:05.482700Z",
     "start_time": "2019-11-06T11:19:45.176446Z"
    },
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "hyperparam_space = {\n",
    "  'C': np.logspace(-12, 12, num=1000, base=2.0),\n",
    "  'gamma': np.logspace(-12, 12, num=1000, base=2.0)\n",
    "}\n",
    "\n",
    "print(hyperparam_space['C'][0:10])\n",
    "\n",
    "svm = BayesianOptimization(\n",
    "  surrogate_model=ProbRandomForestRegressor(n_estimators=100, n_jobs=-1),\n",
    "  objective_model=SVC,\n",
    "  acquisition_function=EI,\n",
    "  hyperparam_space=hyperparam_space,\n",
    "  X_obj=X,\n",
    "  y_obj=y\n",
    ")\n",
    "\n",
    "svm.hyperparam_sampling(n_samples=10)\n",
    "svm.hyperparam_cartesian(size=40)\n",
    "\n",
    "svm.\n",
    "\n",
    "()\n",
    "svm.surrogate_predict()\n",
    "\n",
    "svm.plot_surrogate('(10 Samples)', store_plot=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_space = {\n",
    "  'C': np.logspace(-12, 12, num=1000),\n",
    "  'gamma': np.logspace(-12, 12, num=1000)\n",
    "}            \n",
    "\n",
    "metadata = pd.DataFrame(columns=[*hyperparam_space.keys(), 'loss', 'pred', 'sigma', 'EI'])\n",
    "for hp in hyperparam_space.keys():\n",
    "    metadata[hp] = np.random.choice(hyperparam_space[hp],size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[hyperparam_space.keys()].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(metadata.shape[0]):\n",
    "    clf = SVC(**metadata[hyperparam_space.keys()].iloc[i])\n",
    "    scores = cross_val_score(clf, X, y, cv=3, n_jobs=-1, scoring=\"accuracy\")\n",
    "    metadata.at[i, 'loss'] = np.mean(1 - np.array(scores))\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = points[np.random.choice(1000, n_samples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The black bars indicate the hyperparameter configurations where the objective model was tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XG Boost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T11:48:14.556102Z",
     "start_time": "2019-11-06T11:43:42.108107Z"
    },
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "xgclas_param_template = {\n",
    "  'learning_rate': { 'spacing': 'log', 'min': -4, 'max': -1, 'discrete': False },\n",
    "  'max_depth': { 'spacing': 'lin', 'min': 1, 'max': 6, 'discrete': True }\n",
    "}\n",
    "\n",
    "xgclas = BayesianOptimization(\n",
    "  surrogate_model=ProbRandomForestRegressor(n_estimators=100, n_jobs=-1),\n",
    "  objective_model=XGBClassifier,\n",
    "  acquisition_function=EI,\n",
    "  hyperparam_names=('learning_rate', 'max_depth'),\n",
    "  hyperparam_space=xgclas_param_template,\n",
    "  X_obj=X,\n",
    "  y_obj=y\n",
    ")\n",
    "\n",
    "xgclas.hyperparam_sampling(n_samples=10)\n",
    "xgclas.hyperparam_cartesian(size=40)\n",
    "\n",
    "xgclas.classifier_predict(n_estimators=1000, n_jobs=-1)\n",
    "xgclas.surrogate_predict()\n",
    "\n",
    "xgclas.plot_surrogate(label='(10 Samples)', store_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XG Boost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:08:38.593144Z",
     "start_time": "2019-11-06T12:07:34.657161Z"
    },
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "xgreg_param_template = {\n",
    "  'learning_rate': { 'spacing': 'log', 'min': -4, 'max': -1, 'discrete': False },\n",
    "  'max_depth': { 'spacing': 'lin', 'min': 1, 'max': 6, 'discrete': True }\n",
    "}\n",
    "\n",
    "xgreg = BayesianOptimization(\n",
    "  surrogate_model=ProbRandomForestRegressor(n_estimators=100, n_jobs=-1),\n",
    "  objective_model=XGBRegressor,\n",
    "    \n",
    "  acquisition_function=EI,\n",
    "  hyperparam_names=('learning_rate', 'max_depth'),\n",
    "  hyperparam_space=xgreg_param_template,\n",
    "  X_obj=Xr,\n",
    "  y_obj=yr\n",
    ")\n",
    "\n",
    "xgreg.hyperparam_sampling(n_samples=10)\n",
    "xgreg.hyperparam_cartesian(size=40)\n",
    "\n",
    "xgreg.regressor_predict(n_estimators=1000, n_jobs=-1)\n",
    "xgreg.surrogate_predict()\n",
    "\n",
    "xgreg.plot_surrogate(label='(10 Samples)', store_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ElasticNet Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:41:17.556946Z",
     "start_time": "2019-11-06T12:41:15.065728Z"
    }
   },
   "outputs": [],
   "source": [
    "elas_param_template = {\n",
    "  'alpha': { 'spacing': 'log', 'min': -12, 'max': 12, 'discrete': False },\n",
    "  'l1_ratio': { 'spacing': 'lin', 'min': 1, 'max': 2, 'discrete': False }\n",
    "}\n",
    "\n",
    "elas = BayesianOptimization(\n",
    "  surrogate_model=ProbRandomForestRegressor(n_estimators=100, n_jobs=-1),\n",
    "  objective_model=ElasticNet,\n",
    "  acquisition_function=EI,\n",
    "  hyperparam_names=('alpha', 'l1_ratio'),\n",
    "  hyperparam_space=elas_param_template,\n",
    "  X_obj=Xr,\n",
    "  y_obj=yr\n",
    ")\n",
    "\n",
    "elas.hyperparam_sampling(n_samples=10)\n",
    "elas.hyperparam_cartesian(size=40)\n",
    "\n",
    "elas.regressor_predict()\n",
    "elas.surrogate_predict()\n",
    "\n",
    "elas.plot_surrogate(label='(10 Samples)', store_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize 3 more iterations, each time visualizing the surrogate model and acquisition function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize 3 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T11:21:40.778686Z",
     "start_time": "2019-11-06T11:21:06.183481Z"
    }
   },
   "outputs": [],
   "source": [
    "svm.next_sample()\n",
    "\n",
    "for idx in range(3):\n",
    "    svm.classifier_predict()\n",
    "    svm.surrogate_predict()\n",
    "    svm.plot_surrogate('(Iteration {})'.format(idx+2), store_plot=True)\n",
    "    svm.next_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XG Boost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T11:49:26.218689Z",
     "start_time": "2019-11-06T11:48:14.559322Z"
    },
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "xgclas.next_sample()\n",
    "\n",
    "for idx in range(3):\n",
    "    xgclas.classifier_predict(n_estimators=1000, n_jobs=-1)\n",
    "    xgclas.surrogate_predict()\n",
    "    xgclas.plot_surrogate('(Iteration {})'.format(idx+2), store_plot=True)\n",
    "    xgclas.next_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XG Boost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:09:11.063723Z",
     "start_time": "2019-11-06T12:08:40.252684Z"
    }
   },
   "outputs": [],
   "source": [
    "xgreg.next_sample()\n",
    "\n",
    "for idx in range(3):\n",
    "    xgreg.regressor_predict(n_estimators=1000, n_jobs=-1)\n",
    "    xgreg.surrogate_predict()\n",
    "    xgreg.plot_surrogate('(Iteration {})'.format(idx+2), store_plot=True)\n",
    "    xgreg.next_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ElasticNet Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:41:32.705545Z",
     "start_time": "2019-11-06T12:41:26.839830Z"
    }
   },
   "outputs": [],
   "source": [
    "elas.next_sample()\n",
    "\n",
    "for idx in range(3):\n",
    "    elas.regressor_predict()\n",
    "    elas.surrogate_predict()\n",
    "    elas.plot_surrogate('(Iteration {})'.format(idx+2), store_plot=True)\n",
    "    elas.next_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30 Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T11:30:45.545708Z",
     "start_time": "2019-11-06T11:21:58.923035Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx in range(27):\n",
    "    svm.classifier_predict()\n",
    "    svm.surrogate_predict()\n",
    "    svm.next_sample()\n",
    "    svm.plot_surrogate('(Iteration {})'.format(idx+5), show_plot=False, store_plot=True)\n",
    "\n",
    "svm.generate_gif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:04:00.910684Z",
     "start_time": "2019-11-06T12:04:00.707763Z"
    }
   },
   "outputs": [],
   "source": [
    "svm.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:04:08.982977Z",
     "start_time": "2019-11-06T12:04:08.974653Z"
    }
   },
   "outputs": [],
   "source": [
    "svm.optimal_hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XG Boost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:00:54.251151Z",
     "start_time": "2019-11-06T11:49:26.221616Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx in range(27):\n",
    "    xgclas.classifier_predict(n_estimators=1000, n_jobs=-1)\n",
    "    xgclas.surrogate_predict()\n",
    "    xgclas.next_sample()\n",
    "    xgclas.plot_surrogate('(Iteration {})'.format(idx+5), show_plot=False, store_plot=True)\n",
    "    \n",
    "xgclas.generate_gif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:03:54.955797Z",
     "start_time": "2019-11-06T12:03:54.759176Z"
    }
   },
   "outputs": [],
   "source": [
    "xgclas.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XG Boost Classification optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:04:20.932865Z",
     "start_time": "2019-11-06T12:04:20.925336Z"
    }
   },
   "outputs": [],
   "source": [
    "xgclas.optimal_hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XG Boost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:13:09.591575Z",
     "start_time": "2019-11-06T12:09:13.089999Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx in range(27):\n",
    "    xgreg.regressor_predict(n_estimators=1000, n_jobs=-1)\n",
    "    xgreg.surrogate_predict()\n",
    "    xgreg.next_sample()\n",
    "    xgreg.plot_surrogate('(Iteration {})'.format(idx+5), show_plot=False, store_plot=True)\n",
    "    \n",
    "xgreg.generate_gif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:13:09.796909Z",
     "start_time": "2019-11-06T12:13:09.594360Z"
    }
   },
   "outputs": [],
   "source": [
    "xgreg.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XG Boost Regression optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:13:09.806148Z",
     "start_time": "2019-11-06T12:13:09.799082Z"
    }
   },
   "outputs": [],
   "source": [
    "xgreg.optimal_hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ElasticNet Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:44:15.701877Z",
     "start_time": "2019-11-06T12:41:41.214631Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx in range(27):\n",
    "    elas.regressor_predict()\n",
    "    elas.surrogate_predict()\n",
    "    elas.next_sample()\n",
    "    elas.plot_surrogate('(Iteration {})'.format(idx+5), show_plot=False, store_plot=True)\n",
    "    \n",
    "elas.generate_gif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:44:39.531368Z",
     "start_time": "2019-11-06T12:44:39.313992Z"
    }
   },
   "outputs": [],
   "source": [
    "elas.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ElasticNet optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:44:43.501285Z",
     "start_time": "2019-11-06T12:44:43.493958Z"
    }
   },
   "outputs": [],
   "source": [
    "elas.optimal_hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Discussion & Interpretation \n",
    "\n",
    "Before optimizing the objective model the loss surface is unknown. The real loss surface, in our case, is the same as running a 40x40 grid search over all hyperparameter configurations. However, the computations will be very expensive to run. It order to save on computation cost we apply Bayesian optimization. Bayesian optimization prevents you from searching in locations were the loss is expected to be bad and will probably not improve. This is in stark contrast to grid search, where you need to check every possible hyperparameter conbination. The difference with random search is that random search does not place a prior on the loss function.\n",
    "\n",
    "The more complex the loss surface, the longer it will take the Bayesian optimization to find the best hyperparameters. It will need to conduct more exploration to find possible improvements, before it can exploit. As we can see in the surrogate-loss graphs SVM has a more complex loss function than the other models, this results in the optimizer finding better hyperparameter combinations as it explores and exploits. (You can see this effect in the iterations-loss graph)\n",
    "\n",
    "Especially for simple loss surfaces an optimal hyperparameter setting can be found among the first 10 random samples. Imagine a flat surface (hyperparameters have no influence on the loss), after one random sample you will have found the optimal hyperparameters (you do not need to explore). For the other models we observe a simpler surface then for SVM and therefore there is not much change in the minimum loss (After the initial 10 samples). \n",
    "\n",
    "The efficiency of Bayesian optimization will become more apparent when increasing the amount of hyperparameters to tune. Adding a parameter to your grid search will add an entire additional dimension to the search space. However, with Bayesian optimization you might only need to run it for a few more iterations, depending on the interaction between hyperparameters. \n",
    "\n",
    "The efficiency of Bayesian optimization is also better when you have a model that needs a lot of tuning to achieve better results. As you can see all models except SVM already perform good out of the box. This means our optimization will not improve our models much. \n",
    "\n",
    "Our conclusion is that Bayesian optimization is not that effective in our situation, since for almost every model the optimal hyperparameters were found in the initial 10 random samples. However, when increasing the amount of hyperparameters to tune, Bayesian optimization will become more efficient.\n",
    "\n",
    "(Bayesian optimization works well for both classification and regression, resulting in us not noticing any big differences between the two datasets.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "We decided not to plot the uncertainty interval as it made our plots convoluted and a bit difficult to grasp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:01:57.716652Z",
     "start_time": "2019-11-06T12:01:57.705077Z"
    }
   },
   "source": [
    "## 2. Warm-starting Bayesian Optimization (20 points) {-}\n",
    "\n",
    "* Oh no! 6 of the sensors in the first dataset (robot navigation) suddenly broke. You need to quickly retrain the model but\n",
    "there is no time for a complete re-optimization.\n",
    "* Revisit question 1, but additionally keep a list of the 10 best hyperparameter configurations while you run Bayesian optimization.\n",
    "* Randomly remove 6 columns from the dataset (or remove them manually as long as they are not adjacent) to simulate the broken sensors.\n",
    "* Re-run the Bayesian optimization (only for the first dataset), but now start from the 10 best configurations (for each classifier) rather than 10\n",
    "random ones.\n",
    "* Visualize the surrogate model (as before) at the initial state, and at 3 subsequent iterations.\n",
    "* Interpret and discuss the results. Did the warm-start help? Could you find a good model after a few iterations? \n",
    "Explain the benefits of this approach over starting from scratch or using a random search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 best hyperparameter configurations\n",
    "#### SVM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get 10 best hyperparameter configurations from question 1\n",
    "svm_best_params = svm.optimal_hyperparams(rows=10)\n",
    "svm_best_params.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# set loss to none\n",
    "svm_best_params['loss'] = None\n",
    "svm_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XG Boost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 10 best hyperparameter configurations from question 1\n",
    "xgclas_best_params = xgclas.optimal_hyperparams(rows=10)\n",
    "xgclas_best_params.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# set loss to none\n",
    "xgclas_best_params['loss'] = None\n",
    "xgclas_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly remove 6 columns from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducability\n",
    "np.random.seed(0)\n",
    "\n",
    "# Randomly remove 6 columns, i.e., randomly pick 18 columns to keep \n",
    "kept_columns = np.random.choice(23, 18, replace=False)\n",
    "X_broken = X[:, kept_columns]\n",
    "X_broken.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-run the Bayesian optimization, starting from the 10 best configurations and visualize surrogate model at initial state\n",
    "##### SVM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_broken = BayesianOptimization(\n",
    "    surrogate_model=ProbRandomForestRegressor(n_estimators=100, n_jobs=-1),\n",
    "    objective_model=SVC,\n",
    "    acquisition_function=EI,\n",
    "    hyperparam_names=('C', 'gamma'),\n",
    "    hyperparam_space=svm_param_template,\n",
    "    X_obj=X_broken,\n",
    "    y_obj=y\n",
    ")\n",
    "\n",
    "svm_broken.hyperparams_obj = svm_best_params\n",
    "svm_broken.hyperparam_cartesian(size=30)\n",
    "\n",
    "svm_broken.classifier_predict()\n",
    "svm_broken.surrogate_predict()\n",
    "\n",
    "svm_broken.plot_surrogate('(10 Samples)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XG Boost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgclas_broken = BayesianOptimization(\n",
    "    surrogate_model=ProbRandomForestRegressor(n_estimators=100, n_jobs=-1),\n",
    "    objective_model=XGBClassifier,\n",
    "    acquisition_function=EI,\n",
    "    hyperparam_names=('learning_rate', 'max_depth'),\n",
    "    hyperparam_space=xgclas_param_template,\n",
    "    X_obj=X_broken,\n",
    "    y_obj=y\n",
    ")\n",
    "\n",
    "xgclas_broken.hyperparams_obj = xgclas_best_params\n",
    "xgclas_broken.hyperparam_cartesian(size=30)\n",
    "\n",
    "xgclas_broken.classifier_predict()\n",
    "xgclas_broken.surrogate_predict()\n",
    "\n",
    "xgclas_broken.plot_surrogate(label='(10 Samples)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the surrogate model at 3 subsequent iterations\n",
    "##### SVM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_broken.next_sample()\n",
    "\n",
    "for idx in range(3):\n",
    "    svm_broken.classifier_predict()\n",
    "    svm_broken.surrogate_predict()\n",
    "    svm_broken.plot_surrogate('(Iteration {})'.format(idx+1))\n",
    "    svm_broken.next_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM warm-start optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_broken.optimal_hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XG Boost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgclas_broken.next_sample()\n",
    "\n",
    "for idx in range(3):\n",
    "    xgclas_broken.classifier_predict()\n",
    "    xgclas_broken.surrogate_predict()\n",
    "    xgclas_broken.plot_surrogate('(Iteration {})'.format(idx+1))\n",
    "    xgclas_broken.next_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XG Boost classification warm-start optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgclas_broken.optimal_hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Discussion & Interpretation\n",
    "\n",
    "The warm start did help with finding a good model in few iterations. From the plots of the XG Boost Classification model, you can see that expected improvement dropped rapidly after only a few iterations, with a low loss and expected improvement after the third iteration. From the plots of the SVM model, we do not observe this phenomenon. We expect that the surrogate model is still adjusting to the new data with six broken columns for the first few iterations. Hence, uncertainty is high and therefore we can see a high expected improvement.  \n",
    "\n",
    "The benefits of a warm-start approach over starting from scratch or using a random search are related to the concepts of exploitation versus exploration. Acquisition functions trade off exploitation and exploration, where exploitation means sampling where the surrogate model predicts a low objective loss and exploration means sampling at locations where uncertainty is high. Better initial hyperparameter values, i.e., warm start, encourage Bayesian optimization to prevent exploration and focus on exploitation, hereby finding a good model after fewer iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gaussian Processes (20 points) {-}\n",
    "* Replace the probabilistic Random Forest used above with a Gaussian Process.\n",
    "* Repeat the Bayesian Optimization for one of the datasets, again visualizing the surrogate model and the acquisition function.\n",
    "* If the surrogate models do not look right, do manual tuning\n",
    "- Hint: Try `y_normalize`, regularizing the `alpha` hyperparameter, or trying a different kernel.\n",
    "* Interpret and discuss the results. In which ways are the Gaussian Processes better or worse? Consider both accuracy, speed of finding a good configuration, and runtime. Interpret and explain the results as well as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guassian Process with XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize 10 random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr = GaussianProcessRegressor(normalize_y=False, alpha = 1e-3, n_restarts_optimizer=9)\n",
    "\n",
    "xgreg_gp = BayesianOptimization(\n",
    "  surrogate_model=gpr,\n",
    "  objective_model=XGBRegressor,\n",
    "  acquisition_function=EI,\n",
    "  hyperparam_names=('learning_rate', 'max_depth'),\n",
    "  hyperparam_space=xgreg_param_template,\n",
    "  X_obj=Xr,\n",
    "  y_obj=yr\n",
    ")\n",
    "\n",
    "xgreg_gp.hyperparam_sampling(n_samples=10)\n",
    "xgreg_gp.hyperparam_cartesian(size=30)\n",
    "\n",
    "xgreg_gp.regressor_predict(n_estimators=1000, n_jobs=-1)\n",
    "xgreg_gp.surrogate_predict()\n",
    "\n",
    "xgreg_gp.plot_surrogate(label='(10 Samples)', store_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize 3 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgreg_gp.next_sample()\n",
    "\n",
    "for idx in range(3):\n",
    "    xgreg_gp.regressor_predict(n_estimators=1000, n_jobs=-1)\n",
    "    xgreg_gp.surrogate_predict()\n",
    "    xgreg_gp.plot_surrogate('(Iteration {})'.format(idx+2), store_plot=True)\n",
    "    xgreg_gp.next_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize 30 iterations with a GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(27):\n",
    "    xgreg_gp.regressor_predict(n_estimators=1000, n_jobs=-1)\n",
    "    xgreg_gp.surrogate_predict()\n",
    "    xgreg_gp.next_sample()\n",
    "    xgreg_gp.plot_surrogate('(Iteration {})'.format(idx+5), show_plot=False, store_plot=True)\n",
    "    \n",
    "xgreg_gp.generate_gif()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the minimum loss over 30 iterations with XGBRegressor as objective model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgreg.plot_loss()\n",
    "xgreg_gp.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtime comparison with different surrogate models for XGBRegressor as objective model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_time_sur(model1, model2):\n",
    "    \"\"\"\n",
    "    Compares running time of different surrogate models\n",
    "    \"\"\"\n",
    "    plt.plot(range(1, len(model1.time_surrogate)+1), model1.time_surrogate, label=type(model1.surrogate_model).__name__)\n",
    "    plt.plot(range(1, len(model2.time_surrogate)+1), model2.time_surrogate, label=type(model2.surrogate_model).__name__)\n",
    "    plt.title('Comparison surrogate model runtime of {} vs {} for objective model {}'.\n",
    "              format(type(model1.surrogate_model).__name__, \n",
    "                     type(model2.surrogate_model).__name__,\n",
    "                     model1.model_name),\n",
    "             fontsize=10)\n",
    "    plt.xlabel('iteration nr')\n",
    "    plt.ylabel('runtime (s)')\n",
    "    plt.legend(fontsize=10)\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_time_sur(xgreg, xgreg_gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare optimal hyperparameters for XGBRegressor with GaussianProcessRegressor vs ProbRandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal hyperparameters for {} with {} as surrogate'.format(xgreg.model_name, xgreg.surrogate_model_name))\n",
    "xgreg.optimal_hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal hyperparameters for {} with {} as surrogate'.format(xgreg_gp.model_name, xgreg_gp.surrogate_model_name))\n",
    "xgreg_gp.optimal_hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guassian Process with ElasticNet Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize 10 random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:45:38.658553Z",
     "start_time": "2019-11-06T12:45:36.909120Z"
    }
   },
   "outputs": [],
   "source": [
    "gpr = GaussianProcessRegressor(normalize_y=False, alpha = 1e-3, n_restarts_optimizer=9)\n",
    "\n",
    "elas_gpr = BayesianOptimization(\n",
    "  surrogate_model=gpr,\n",
    "  objective_model=ElasticNet,\n",
    "  acquisition_function=EI,\n",
    "  hyperparam_names=('alpha', 'l1_ratio'),\n",
    "  hyperparam_space=elas_param_template,\n",
    "  X_obj=Xr,\n",
    "  y_obj=yr\n",
    ")\n",
    "\n",
    "elas_gpr.hyperparam_sampling(n_samples=10)\n",
    "elas_gpr.hyperparam_cartesian(size=30)\n",
    "\n",
    "elas_gpr.regressor_predict()\n",
    "elas_gpr.surrogate_predict()\n",
    "\n",
    "elas_gpr.plot_surrogate(label='(10 Samples)', store_plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize 3 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:46:00.895475Z",
     "start_time": "2019-11-06T12:45:52.936655Z"
    }
   },
   "outputs": [],
   "source": [
    "elas_gpr.next_sample()\n",
    "\n",
    "for idx in range(3):\n",
    "    elas_gpr.regressor_predict()\n",
    "    elas_gpr.surrogate_predict()\n",
    "    elas_gpr.plot_surrogate('(Iteration {})'.format(idx+2), store_plot=True)\n",
    "    elas_gpr.next_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize 30 iterations with GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T12:48:40.037975Z",
     "start_time": "2019-11-06T12:47:15.453016Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx in range(27):\n",
    "    elas_gpr.regressor_predict()\n",
    "    elas_gpr.surrogate_predict()\n",
    "    elas_gpr.next_sample()\n",
    "    elas_gpr.plot_surrogate('(Iteration {})'.format(idx+5), show_plot=False, store_plot=True)\n",
    "    \n",
    "elas_gpr.generate_gif()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare loss xgboost regression RFR & GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elas.plot_loss()\n",
    "elas_gpr.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare times xgboost regression RFR & GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_time_sur(elas, elas_gpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare optimal hyperparameters for ElasticNet with GaussianProcessRegressor vs ProbRandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal hyperparameters for {} with {} as surrogate'.format(elas.model_name, elas.surrogate_model_name))\n",
    "elas.optimal_hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal hyperparameters for {} with {} as surrogate'.format(elas_gpr.model_name, elas_gpr.surrogate_model_name))\n",
    "elas_gpr.optimal_hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Discussion & Interpretation\n",
    "\n",
    "* Tuning the GaussianProcessRegressor:\n",
    "    - __y-norm__: Did not visibly affect the surface or performance of the surrogate model in this particular situation. Scikit learn suggests to set y-norm to True if the target values’ mean is expected to differ considerable from zero.\n",
    "    - __alpha__: Alpha is the regularization parameter and had clear effect on performance of the surrogate model. For the XGBRegressor the default value of alpha (1e-10) led to an error in the code. The Scikit learn documentation suggests this could be the result of numerical issues during the fitting process. With alpha = 1e-3, the surface of the surrogate model behaved as expected.\n",
    "    - __n_restarts_optimizer__: Again, did not seem to have an effect on the performance of the surrogate model in this particular situation.\n",
    "    - __kernel__: Highly affected the performance of the surrogate model. Default kernel was used this situation as it resulted in a nice surface.\n",
    "    \n",
    "\n",
    "* First of all, it immediately stands out that the different surrogate models behave quite differently. This is particularly true in the first scenario where XGBRegressor is the objective model. Here, the ProbRandomForestRegressor (PRFR) results in a blocky, slide-looking surface for which the max_depth hyperparameter seems to have little influence on the loss. In contrast, the GaussianProcessRegressor (GPR) is a smooth, wavy-looking surface where both hyperparameters clearly affect the loss. Despite the different surfaces, both surrogate models result in similar values for the optimal hyperparameters. Hence the type of surrogate model does not seem to be an important factor for overall accuracy for this dataset. \n",
    "\n",
    "* We can draw a similar conclusion looking at the plot that tracks the minimum observed loss over all the iterations of the bayesian optimization process. While there might be a minor improvement over the 30 iterations, generally all surrogate models tend to find a near optimal configuration with just 10 initial random samples. Note the small scale for the y-axis of the minimum loss graphs, this makes an improvement look deceptively large even though it only represents a small change. \n",
    "\n",
    "* Lastly, there is only a marginal difference in the runtime between PRFR and GPR as can be seen in the plots above. The runtime reflects the time it takes each iteration to fit the surrogate model and to make predictions on the given sample. Particularly in contrast to the running times of the objective models, both surrogate models are much more efficient in locating an ideal hyperparameter configuration. \n",
    "\n",
    "* Overall we can conclude that PRFR and GPR are equally as good in terms of surrogate models. The only noticable distinction between the two is that GPR requires more manual tuning and has smoother output surfaces. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "262px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
